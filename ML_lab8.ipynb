{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4pZtOqqzcYtD",
        "outputId": "1054d3c2-2516-4255-9acc-5a35009c5a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root node feature index: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DecisionTreeRootNodeDetector:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def entropy(self, labels):\n",
        "        _, counts = np.unique(labels, return_counts=True)\n",
        "        probabilities = counts / len(labels)\n",
        "        return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "    def information_gain(self, parent_labels, splits_labels):\n",
        "        parent_entropy = self.entropy(parent_labels)\n",
        "        splits_entropy = 0\n",
        "        total_samples = sum(len(split) for split in splits_labels)\n",
        "        for split_labels in splits_labels:\n",
        "            split_weight = len(split_labels) / total_samples\n",
        "            splits_entropy += split_weight * self.entropy(split_labels)\n",
        "        return parent_entropy - splits_entropy\n",
        "\n",
        "    def find_root_node(self, features, labels):\n",
        "        best_info_gain = -1\n",
        "        best_feature_index = None\n",
        "\n",
        "        for feature_index in range(features.shape[1]):\n",
        "            feature_values = features[:, feature_index]\n",
        "            unique_values = np.unique(feature_values)\n",
        "            splits_labels = []\n",
        "            for value in unique_values:\n",
        "                split_indices = np.where(feature_values == value)[0]\n",
        "                split_labels = labels[split_indices]\n",
        "                splits_labels.append(split_labels)\n",
        "\n",
        "            info_gain = self.information_gain(labels, splits_labels)\n",
        "            if info_gain > best_info_gain:\n",
        "                best_info_gain = info_gain\n",
        "                best_feature_index = feature_index\n",
        "\n",
        "        return best_feature_index\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/new_dataset.csv\")  # Replace 'your_dataset.csv' with the path to your CSV file\n",
        "\n",
        "# Convert non-numeric columns to strings\n",
        "for col in dataset.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(dataset[col]):\n",
        "        dataset[col] = dataset[col].astype(str)\n",
        "\n",
        "# Extract features and labels\n",
        "features = dataset.drop(columns=['indicator']).values\n",
        "labels = dataset['indicator'].values\n",
        "\n",
        "# Initialize and use the DecisionTreeRootNodeDetector\n",
        "detector = DecisionTreeRootNodeDetector()\n",
        "root_node_index = detector.find_root_node(features, labels)\n",
        "print(\"Root node feature index:\", root_node_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DecisionTreeRootNodeDetector:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def bin_continuous_feature(self, feature_values, binning_type='equal_width', num_bins=10):\n",
        "        binned_values = np.zeros_like(feature_values, dtype=int)  # Initialize array for binned values\n",
        "\n",
        "        for i, column in enumerate(feature_values.T):  # Iterate over each feature column\n",
        "            numeric_values = column.astype(float)  # Convert to float to handle non-numeric values\n",
        "\n",
        "            if binning_type == 'equal_width':\n",
        "                min_value = np.min(numeric_values)\n",
        "                max_value = np.max(numeric_values)\n",
        "                bins = np.linspace(min_value, max_value, num_bins + 1)\n",
        "            elif binning_type == 'frequency':\n",
        "                bins = np.quantile(numeric_values, np.linspace(0, 1, num_bins + 1))\n",
        "            else:\n",
        "                raise ValueError(\"Invalid binning type. Choose 'equal_width' or 'frequency'.\")\n",
        "\n",
        "            binned_values[:, i] = np.digitize(numeric_values, bins) - 1  # Subtract 1 to start bin indexing from 0\n",
        "\n",
        "        return binned_values\n",
        "\n",
        "# Load C code embeddings dataset\n",
        "dataset = pd.read_csv(\"/content/new_dataset.csv\")  # Adjust the path to your dataset\n",
        "\n",
        "# Convert the entire dataset to features\n",
        "features = dataset.drop(columns=['indicator']).values\n",
        "\n",
        "# Initialize DecisionTreeRootNodeDetector\n",
        "detector = DecisionTreeRootNodeDetector()\n",
        "\n",
        "# Binning example\n",
        "binned_values = detector.bin_continuous_feature(features, binning_type='equal_width', num_bins=10)\n",
        "print(\"Binned values (Equal Width Binning):\", binned_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qDCgVXC5cJB",
        "outputId": "c2b0d8cd-1787-4bf1-a9fc-0f9ae077516a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binned values (Equal Width Binning): [[ 5  8  5 ... 10 10 10]\n",
            " [ 5  7  5 ... 10 10 10]\n",
            " [ 5  8  5 ... 10 10 10]\n",
            " ...\n",
            " [ 3  1  6 ... 10 10 10]\n",
            " [ 2  1  5 ... 10 10 10]\n",
            " [ 3  1  5 ... -1 -1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree_ = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(inputs) for inputs in X]\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
        "           (num_classes == 1) or \\\n",
        "           (num_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return {'leaf': True, 'value': leaf_value}\n",
        "\n",
        "        best_feature_index, best_threshold = self._find_best_split(X, y)\n",
        "\n",
        "        if best_feature_index is None:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return {'leaf': True, 'value': leaf_value}\n",
        "\n",
        "        left_indices = np.where(X[:, best_feature_index] <= best_threshold)[0]\n",
        "        right_indices = np.where(X[:, best_feature_index] > best_threshold)[0]\n",
        "\n",
        "        left_subtree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
        "\n",
        "        return {'leaf': False, 'feature_index': best_feature_index, 'threshold': best_threshold,\n",
        "                'left': left_subtree, 'right': right_subtree}\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_info_gain = -1\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = X[:, feature_index]\n",
        "            unique_values = np.unique(feature_values)\n",
        "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(feature_values <= threshold)[0]\n",
        "                right_indices = np.where(feature_values > threshold)[0]\n",
        "\n",
        "                if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                splits_labels = [y[left_indices], y[right_indices]]\n",
        "                info_gain = self._information_gain(y, splits_labels)\n",
        "\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature_index, best_threshold\n",
        "\n",
        "    def _information_gain(self, parent_labels, splits_labels):\n",
        "        parent_entropy = self._entropy(parent_labels)\n",
        "        splits_entropy = 0\n",
        "        total_samples = sum(len(split) for split in splits_labels)\n",
        "        for split_labels in splits_labels:\n",
        "            split_weight = len(split_labels) / total_samples\n",
        "            splits_entropy += split_weight * self._entropy(split_labels)\n",
        "        return parent_entropy - splits_entropy\n",
        "\n",
        "    def _entropy(self, labels):\n",
        "        _, counts = np.unique(labels, return_counts=True)\n",
        "        probabilities = counts / len(labels)\n",
        "        return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "    def _most_common_label(self, labels):\n",
        "        return np.bincount(labels).argmax()\n",
        "\n",
        "    def _predict(self, inputs):\n",
        "        node = self.tree_\n",
        "        while not node['leaf']:\n",
        "            if inputs[node['feature_index']] <= node['threshold']:\n",
        "                node = node['left']\n",
        "            else:\n",
        "                node = node['right']\n",
        "        return node['value']\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(\"/content/new_dataset.csv\")  # Adjust the path to your dataset\n",
        "\n",
        "# Display first few rows of the dataset to understand its structure\n",
        "print(dataset.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in dataset:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "# Convert non-numeric columns to numeric\n",
        "dataset['indicator'] = dataset['indicator'].map({'code_only': 0, 'code_comm': 1})\n",
        "\n",
        "# Extract features and labels\n",
        "X = dataset.drop(columns=['indicator']).values\n",
        "y = dataset['indicator'].values\n",
        "\n",
        "# Initialize DecisionTree\n",
        "tree = DecisionTree()\n",
        "\n",
        "# Fit the model\n",
        "tree.fit(X, y)\n",
        "\n",
        "# Example prediction\n",
        "example_input = X[0]\n",
        "predicted_label = tree.predict([example_input])\n",
        "print(\"Predicted label:\", predicted_label[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpDXNlJx5vtU",
        "outputId": "614e14d8-c823-4dd7-ce6e-20308745c406"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6  \\\n",
            "0 -0.560180  0.397133 -0.067608 -1.361568 -1.189112  0.362005 -2.113351   \n",
            "1 -0.617345  0.004933  0.107997 -1.275459 -1.116872  0.391739 -2.048924   \n",
            "2 -0.597761  0.410195 -0.095720 -1.338432 -1.206770  0.355120 -2.098167   \n",
            "3 -0.546645  0.208720 -0.045373 -1.222537 -1.078728  0.421885 -2.104985   \n",
            "4 -0.643125  0.430050 -0.008826 -1.351897 -1.171904  0.367173 -2.125630   \n",
            "\n",
            "          7         8         9  ...       760       761       762       763  \\\n",
            "0 -0.945830  0.967215 -1.035563  ... -2.182985 -1.788343 -1.500597  0.575761   \n",
            "1 -1.050489  0.850765 -1.035608  ... -2.184924 -1.784352 -1.194703  0.194767   \n",
            "2 -0.965952  0.973628 -1.008978  ... -2.188181 -1.790508 -1.496068  0.613387   \n",
            "3 -1.122851  0.968126 -0.895802  ... -2.176035 -1.723567 -1.477411  0.382508   \n",
            "4 -0.877730  0.865652 -1.048714  ... -2.250539 -1.726540 -1.531785  0.549569   \n",
            "\n",
            "        764       765       766       767  score  indicator  \n",
            "0 -1.418272  1.969096 -1.663663  0.929276   10.0  code_only  \n",
            "1 -1.383189  2.073854 -1.415486  0.632933    8.0  code_only  \n",
            "2 -1.429072  1.965157 -1.676064  0.918330    7.0  code_only  \n",
            "3 -1.410528  1.997823 -1.583785  0.787734    5.0  code_only  \n",
            "4 -1.434322  2.015190 -1.620357  1.025025    6.0  code_only  \n",
            "\n",
            "[5 rows x 770 columns]\n",
            "Missing values in dataset:\n",
            "0            0\n",
            "1            0\n",
            "2            0\n",
            "3            0\n",
            "4            0\n",
            "            ..\n",
            "765          3\n",
            "766          3\n",
            "767          3\n",
            "score        7\n",
            "indicator    3\n",
            "Length: 770, dtype: int64\n",
            "Predicted label: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}